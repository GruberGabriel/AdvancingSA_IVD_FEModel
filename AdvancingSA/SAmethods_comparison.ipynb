{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto-reloading for functions during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from SALib.sample import saltelli, fast_sampler\n",
    "from SALib.analyze import sobol, fast, rbd_fast\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "X = pd.read_csv(\"./simulation_data/MaterialConfigurations_1024.csv\")\n",
    "Y_rom_files = {\n",
    "    \"ROM Flexion\": \"./simulation_data/ROM_Results_Flexion_wMoment_1024.csv\",\n",
    "    \"ROM Extension\": \"./simulation_data/ROM_Results_Extension_wMoment_1024.csv\",\n",
    "    \"ROM Lateral Bending\": \"./simulation_data/ROM_Results_LateralBending_wMoment_1024.csv\",\n",
    "    \"ROM Axial Rotation\": \"./simulation_data/ROM_Results_AxialRotation_wMoment_1024.csv\"\n",
    "}\n",
    "Y_idp_files = {\n",
    "    \"IDP Flexion\": \"./simulation_data/IDP_Results_Flexion_wMoment_1024.csv\",\n",
    "    \"IDP Extension\": \"./simulation_data/IDP_Results_Extension_wMoment_1024.csv\",\n",
    "    \"IDP Lateral Bending\": \"./simulation_data/IDP_Results_LateralBending_wMoment_1024.csv\",\n",
    "    \"IDP Axial Rotation\": \"./simulation_data/IDP_Results_AxialRotation_wMoment_1024.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model-based SA methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def load_last_column(file_path):\n",
    "    \"\"\"Loads the last column of a CSV file as a Series.\"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data.iloc[:, -1]\n",
    "\n",
    "def scale_data(df):\n",
    "    \"\"\"Applies min-max scaling to a DataFrame.\"\"\"\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "def perform_regression(X, y):\n",
    "    \"\"\"Performs linear regression and returns model, coefficients, intercept, and R² score.\"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    return model, model.coef_, model.intercept_, r2\n",
    "\n",
    "def calculate_r2_for_parameters(X, y):\n",
    "    \"\"\"Calculates R² for each parameter individually.\"\"\"\n",
    "    r2_scores = {}\n",
    "    for param in X.columns:\n",
    "        model = LinearRegression()\n",
    "        model.fit(X[[param]], y)\n",
    "        r2_scores[param] = model.score(X[[param]], y)\n",
    "    return r2_scores\n",
    "\n",
    "def calculate_pearson_correlations(X, y):\n",
    "    \"\"\"Calculates Pearson's correlation and p-value for each parameter.\"\"\"\n",
    "    correlations, p_values = {}, {}\n",
    "    for param in X.columns:\n",
    "        correlation, p_value = pearsonr(X[param], y)\n",
    "        correlations[param] = correlation\n",
    "        p_values[param] = p_value\n",
    "    return correlations, p_values\n",
    "\n",
    "def load_car_scores(filename):\n",
    "    \"\"\"Loads CAR scores from a CSV file.\"\"\"\n",
    "    return pd.read_csv(filename).iloc[:, 0].values\n",
    "\n",
    "def save_dict_to_csv(data, filename, index_name=\"Parameter\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.index.name = index_name\n",
    "    df.to_csv(filename)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed. Results are stored in the 'results' dictionary.\n"
     ]
    }
   ],
   "source": [
    "# Scale Data\n",
    "X_scaled = scale_data(X)\n",
    "Y_rom = {name: scale_data(load_last_column(path)) for name, path in Y_rom_files.items()}\n",
    "Y_idp = {name: scale_data(load_last_column(path)) for name, path in Y_idp_files.items()}\n",
    "\n",
    "# Linear Regression Analysis\n",
    "results = {}\n",
    "for name, y in {**Y_rom, **Y_idp}.items():\n",
    "    model, coefs, intercept, r2 = perform_regression(X_scaled, y)\n",
    "    r2_params = calculate_r2_for_parameters(X_scaled, y)\n",
    "    correlation, _ = calculate_pearson_correlations(X_scaled, y)\n",
    "    results[name] = {\n",
    "        \"model\": model,\n",
    "        \"coefficients\": coefs,\n",
    "        \"intercept\": intercept,\n",
    "        \"r2_score\": r2,\n",
    "        \"r2_params\": r2_params,\n",
    "        \"pearson_correlation\": correlation\n",
    "    }\n",
    "\n",
    "# Load CAR Scores for Comparison\n",
    "car_score_files = {\n",
    "    \"ROM Flexion\": \"./results_data/CARScores_ROM_flexion.csv\",\n",
    "    \"ROM Extension\": \"./results_data/CARScores_ROM_extension.csv\",\n",
    "    \"ROM Lateral Bending\": \"./results_data/CARScores_ROM_lateral_bending.csv\",\n",
    "    \"ROM Axial Rotation\": \"./results_data/CARScores_ROM_axial_rotation.csv\",\n",
    "    \"IDP Flexion\": \"./results_data/CARScores_IDP_flexion.csv\",\n",
    "    \"IDP Extension\": \"./results_data/CARScores_IDP_extension.csv\",\n",
    "    \"IDP Lateral Bending\": \"./results_data/CARScores_IDP_lateral_bending.csv\",\n",
    "    \"IDP Axial Rotation\": \"./results_data/CARScores_IDP_axial_rotation.csv\"\n",
    "}\n",
    "car_scores = {name: load_car_scores(path) for name, path in car_score_files.items()}\n",
    "\n",
    "# Summary Ratios\n",
    "for name in results:\n",
    "    r2_total = results[name][\"r2_score\"]\n",
    "    car_scores_squared = car_scores[name]**2\n",
    "    results[name][\"r2_ratios\"] = {param: r2 / r2_total for param, r2 in results[name][\"r2_params\"].items()}\n",
    "    results[name][\"car_ratios\"] = car_scores_squared / r2_total\n",
    "\n",
    "# Results contain all necessary information for further analysis or visualization\n",
    "print(\"Analysis completed. Results are stored in the 'results' dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved successfully to './results_data/' directory.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the results directory exists\n",
    "output_dir = \"./results_data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Extract and store CAR²-ratios, R² ratios, and Pearson's correlation coefficients\n",
    "car_ratios_data = {}\n",
    "r2_ratios_data = {}\n",
    "pearson_correlations_data = {}\n",
    "\n",
    "# Populate dictionaries with extracted results for each response variable\n",
    "for name, result in results.items():\n",
    "    car_ratios_data[name] = result[\"car_ratios\"]\n",
    "    r2_ratios_data[name] = result[\"r2_ratios\"]\n",
    "    pearson_correlations_data[name] = result[\"pearson_correlation\"]\n",
    "\n",
    "# Convert dictionaries to DataFrames and save as CSV files\n",
    "save_dict_to_csv(car_ratios_data, os.path.join(output_dir, \"CAR_squared_ratios.csv\"))\n",
    "save_dict_to_csv(r2_ratios_data, os.path.join(output_dir, \"COD_ratios.csv\"))\n",
    "save_dict_to_csv(pearson_correlations_data, os.path.join(output_dir, \"Pearson_correlation_coefficients.csv\"))\n",
    "\n",
    "print(\"Files saved successfully to './results_data/' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance-based SA methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ge59dic\\AppData\\Local\\Temp\\ipykernel_3232\\2021152038.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(file_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Device setup for neural network operations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters for the neural network\n",
    "hparams = {\n",
    "    'input_dim': 15,\n",
    "    'output_dim': 1,\n",
    "    'num_units': 256,\n",
    "    'num_layers': 5,\n",
    "    'dropout_p': 0\n",
    "}\n",
    "\n",
    "# Neural Network Model Definition\n",
    "class RegressionNet(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(RegressionNet, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(hparams['num_layers'] - 1):\n",
    "            i_dim = hparams['input_dim'] if i == 0 else hparams['num_units'] // (2 ** (i - 1))\n",
    "            o_dim = hparams['num_units'] // (2 ** i)\n",
    "            layers.append(nn.Linear(i_dim, o_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=hparams['dropout_p']))\n",
    "        layers.append(nn.Linear(o_dim, hparams['output_dim']))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Load Pretrained Neural Network Models\n",
    "def load_nn_model(file_path, hparams):\n",
    "    model = RegressionNet(hparams)\n",
    "    model.load_state_dict(torch.load(file_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Paths to neural network models\n",
    "nn_model_rom = load_nn_model('./models/nn_model_ROM.pth', hparams)\n",
    "nn_model_idp = load_nn_model('./models/nn_model_IDP.pth', hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobol analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ge59dic\\AppData\\Local\\Temp\\ipykernel_3232\\3479409787.py:9: DeprecationWarning: `salib.sample.saltelli` will be removed in SALib 1.5.1 Please use `salib.sample.sobol`\n",
      "  param_values = saltelli.sample(problem, num_samples, calc_second_order=True)\n",
      "c:\\Users\\ge59dic\\anaconda3\\envs\\sensitivity_analysis\\lib\\site-packages\\SALib\\util\\__init__.py:274: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  names = list(pd.unique(groups))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sobol analysis completed. Results saved to './results_data/'.\n"
     ]
    }
   ],
   "source": [
    "# Sobol Analysis Setup\n",
    "problem = {\n",
    "    'num_vars': X.shape[1],  # Number of input variables,\n",
    "    'names': X.columns.tolist(),  # Input variable names\n",
    "    'bounds': [[0, 1]] * X.shape[1]  # Normalized bounds\n",
    "}\n",
    "\n",
    "num_samples = 2**13  # To reach convergence\n",
    "param_values = saltelli.sample(problem, num_samples, calc_second_order=True)\n",
    "\n",
    "# Generate Load Cases for Sobol Analysis\n",
    "def generate_load_cases(base_params):\n",
    "    load_cases = {\n",
    "        \"flexion\": np.concatenate([np.zeros((base_params.shape[0], 1)), np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"extension\": np.concatenate([np.ones((base_params.shape[0], 1)) * 1/3, np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"lateral_bending\": np.concatenate([np.ones((base_params.shape[0], 1)) * 2/3, np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"axial_rotation\": np.concatenate([np.ones((base_params.shape[0], 1)), np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1)\n",
    "    }\n",
    "    return {case: torch.tensor(params, dtype=torch.float32, device=device) for case, params in load_cases.items()}\n",
    "\n",
    "param_values_torch = generate_load_cases(param_values)\n",
    "\n",
    "# Predictions Using Neural Network Models\n",
    "def predict_with_model(model, param_values):\n",
    "    predictions = {}\n",
    "    with torch.no_grad():\n",
    "        for case, params in param_values.items():\n",
    "            predictions[case] = model(params).cpu().numpy().squeeze()\n",
    "    return predictions\n",
    "\n",
    "# Perform Sobol Analysis\n",
    "def perform_sobol_analysis(problem, predictions):\n",
    "    sobol_results = {}\n",
    "    for case, preds in predictions.items():\n",
    "        Si = sobol.analyze(problem, preds, num_resamples=1000, print_to_console=False)\n",
    "        sobol_results[case] = Si\n",
    "    return sobol_results\n",
    "\n",
    "# Store Sobol Results\n",
    "def sobol_results_to_csv(sobol_results, metric):\n",
    "    output_dir = './results_data/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for case, Si in sobol_results.items():\n",
    "        # Single-order and total-order indices\n",
    "        df_1d = pd.DataFrame({\n",
    "            'S1': Si['S1'],\n",
    "            'S1_conf': Si['S1_conf'],\n",
    "            'ST': Si['ST'],\n",
    "            'ST_conf': Si['ST_conf'],\n",
    "        }, index=problem['names'])\n",
    "        \n",
    "        # Second-order indices\n",
    "        df_s2 = pd.DataFrame(Si['S2'], index=problem['names'], columns=problem['names'])\n",
    "        df_s2_conf = pd.DataFrame(Si['S2_conf'], index=problem['names'], columns=problem['names'])\n",
    "        \n",
    "        # Paths for saving the results with \"sobol\" in the filename\n",
    "        path_s1st = f\"{output_dir}SobolS1ST_{metric}_{case}.csv\"\n",
    "        path_s2 = f\"{output_dir}SobolS2_{metric}_{case}.csv\"\n",
    "        path_s2_conf = f\"{output_dir}SobolS2conf_{metric}_{case}.csv\"\n",
    "        \n",
    "        # Save dataframes to CSV\n",
    "        df_1d.to_csv(path_s1st)\n",
    "        df_s2.to_csv(path_s2)\n",
    "        df_s2_conf.to_csv(path_s2_conf)\n",
    "\n",
    "# Execute Sobol Analysis for ROM and IDP Metrics\n",
    "rom_predictions = predict_with_model(nn_model_rom, param_values_torch)\n",
    "idp_predictions = predict_with_model(nn_model_idp, param_values_torch)\n",
    "\n",
    "rom_sobol_results = perform_sobol_analysis(problem, rom_predictions)\n",
    "idp_sobol_results = perform_sobol_analysis(problem, idp_predictions)\n",
    "\n",
    "# Save results to CSV files\n",
    "sobol_results_to_csv(rom_sobol_results, \"ROM\")\n",
    "sobol_results_to_csv(idp_sobol_results, \"IDP\")\n",
    "\n",
    "print(\"Sobol analysis completed. Results saved to './results_data/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eFAST analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eFAST analysis completed. Results saved to './results_data/'.\n"
     ]
    }
   ],
   "source": [
    "# Define eFAST Analysis Problem Setup\n",
    "problem = {\n",
    "    'num_vars': X.shape[1],  # Number of input variables,\n",
    "    'names': X.columns.tolist(),  # Input variable names\n",
    "    'bounds': [[0, 1]] * X.shape[1]  # Normalized bounds\n",
    "}\n",
    "\n",
    "# Number of samples for eFAST\n",
    "M_fast = 4\n",
    "num_samples = 2**13  # Ensures sufficient convergence\n",
    "\n",
    "# Generate eFAST Samples\n",
    "param_values = fast_sampler.sample(problem, num_samples, M=M_fast, seed=4)\n",
    "\n",
    "# Generate Load Cases for eFAST Analysis\n",
    "def generate_load_cases(base_params):\n",
    "    \"\"\"Generates parameters for each load case.\"\"\"\n",
    "    load_cases = {\n",
    "        \"flexion\": np.concatenate([np.zeros((base_params.shape[0], 1)), np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"extension\": np.concatenate([np.ones((base_params.shape[0], 1)) * 1/3, np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"lateral_bending\": np.concatenate([np.ones((base_params.shape[0], 1)) * 2/3, np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"axial_rotation\": np.concatenate([np.ones((base_params.shape[0], 1)), np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1)\n",
    "    }\n",
    "    return {case: torch.tensor(params, dtype=torch.float32, device=device) for case, params in load_cases.items()}\n",
    "\n",
    "# Device setup for neural network operations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load cases for eFAST\n",
    "param_values_torch = generate_load_cases(param_values)\n",
    "\n",
    "# Predictions Using Neural Network Models\n",
    "def predict_with_model(model, param_values):\n",
    "    \"\"\"Generates predictions for each load case using the neural network model.\"\"\"\n",
    "    predictions = {}\n",
    "    with torch.no_grad():\n",
    "        for case, params in param_values.items():\n",
    "            predictions[case] = model(params).cpu().numpy().squeeze()\n",
    "    return predictions\n",
    "\n",
    "# Perform eFAST Analysis\n",
    "def perform_fast_analysis(problem, predictions):\n",
    "    \"\"\"Performs eFAST analysis and stores results for each load case.\"\"\"\n",
    "    fast_results = {}\n",
    "    for case, preds in predictions.items():\n",
    "        Si = fast.analyze(problem, preds, M=M_fast, num_resamples=1000, conf_level=0.95, print_to_console=False, seed=4)\n",
    "        fast_results[case] = Si\n",
    "    return fast_results\n",
    "\n",
    "# Store eFAST Results\n",
    "def fast_results_to_csv(fast_results, metric):\n",
    "    \"\"\"Converts and saves eFAST analysis results to CSV files.\"\"\"\n",
    "    output_dir = './results_data/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for case, Si in fast_results.items():\n",
    "        # First-order and total-order indices\n",
    "        df_1d = pd.DataFrame({\n",
    "            'S1': Si['S1'],\n",
    "            'S1_conf': Si['S1_conf'],\n",
    "            'ST': Si['ST'],\n",
    "            'ST_conf': Si['ST_conf'],\n",
    "        }, index=problem['names'])\n",
    "        \n",
    "        # Path for saving the results\n",
    "        path_s1st = f\"{output_dir}EFAST_{metric}_{case}.csv\"\n",
    "        \n",
    "        # Save dataframe to CSV\n",
    "        df_1d.to_csv(path_s1st)\n",
    "\n",
    "# Execute eFAST Analysis for ROM and IDP Metrics\n",
    "rom_predictions = predict_with_model(nn_model_rom, param_values_torch)\n",
    "idp_predictions = predict_with_model(nn_model_idp, param_values_torch)\n",
    "\n",
    "rom_fast_results = perform_fast_analysis(problem, rom_predictions)\n",
    "idp_fast_results = perform_fast_analysis(problem, idp_predictions)\n",
    "\n",
    "# Save results to CSV files\n",
    "fast_results_to_csv(rom_fast_results, \"ROM\")\n",
    "fast_results_to_csv(idp_fast_results, \"IDP\")\n",
    "\n",
    "print(\"eFAST analysis completed. Results saved to './results_data/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBD-FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flexion\n",
      "extension\n",
      "lateral_bending\n",
      "axial_rotation\n",
      "flexion\n",
      "extension\n",
      "lateral_bending\n",
      "axial_rotation\n",
      "RBD-FAST analysis completed. Results saved in './results_data/'.\n"
     ]
    }
   ],
   "source": [
    "# Define the RBD-FAST Analysis Problem Setup\n",
    "problem = {\n",
    "    'num_vars': X.shape[1],  # Number of input variables,\n",
    "    'names': X.columns.tolist(),  # Input variable names\n",
    "    'bounds': [[0, 1]] * X.shape[1]  # Normalized bounds\n",
    "}\n",
    "\n",
    "# Load response variables for ROM and IDP\n",
    "Y_rom = {case: pd.read_csv(path).iloc[:, -1] for case, path in Y_rom_files.items()}\n",
    "Y_idp = {case: pd.read_csv(path).iloc[:, -1] for case, path in Y_idp_files.items()}\n",
    "\n",
    "# Perform RBD-FAST Analysis\n",
    "def perform_rbd_fast_analysis(problem, X, y_dict):\n",
    "    \"\"\"Performs RBD-FAST analysis for each response variable in different load cases.\"\"\"\n",
    "    rbd_fast_results = {}\n",
    "    for case, y in y_dict.items():\n",
    "        Si = rbd_fast.analyze(problem, X=X.values, Y=y.values, print_to_console=False)\n",
    "        rbd_fast_results[case] = Si\n",
    "    return rbd_fast_results\n",
    "\n",
    "# Store RBD-FAST Results\n",
    "def rbd_fast_results_to_csv(rbd_fast_results, metric):\n",
    "    \"\"\"Saves RBD-FAST results to CSV files for each load case.\"\"\"\n",
    "    output_dir = './results_data/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for case, Si in rbd_fast_results.items():\n",
    "        # Extract load case by removing the metric prefix (\"ROM\" or \"IDP\") and joining the rest\n",
    "        load_case = \"_\".join(case.split()[1:]).lower().replace(\" \", \"\")  # Preserve full load case\n",
    "        \n",
    "        # Create the filename with the desired format\n",
    "        filename = f\"RBDFAST_{metric}_{load_case}.csv\"\n",
    "        \n",
    "        print(load_case)\n",
    "        df_results = pd.DataFrame({\n",
    "            'S1': Si['S1'],\n",
    "            'S1_conf': Si['S1_conf']\n",
    "        }, index=problem['names'])\n",
    "\n",
    "        # Save results to the specified path\n",
    "        df_results.to_csv(os.path.join(output_dir, filename))\n",
    "\n",
    "# Execute RBD-FAST Analysis for ROM and IDP Metrics\n",
    "rom_rbd_fast_results = perform_rbd_fast_analysis(problem, X, Y_rom)\n",
    "idp_rbd_fast_results = perform_rbd_fast_analysis(problem, X, Y_idp)\n",
    "\n",
    "# Save results to CSV files\n",
    "rbd_fast_results_to_csv(rom_rbd_fast_results, \"ROM\")\n",
    "rbd_fast_results_to_csv(idp_rbd_fast_results, \"IDP\")\n",
    "\n",
    "print(\"RBD-FAST analysis completed. Results saved in './results_data/'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensitivity_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
