{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto-reloading for functions during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from SALib.sample import saltelli, fast_sampler\n",
    "from SALib.analyze import sobol, fast, rbd_fast\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "X = pd.read_csv(\"./simulation_data/MaterialConfigurations_1024.csv\")\n",
    "Y_rom_files = {\n",
    "    \"ROM Flexion\": \"./simulation_data/ROM_Results_Flexion_wMoment_1024.csv\",\n",
    "    \"ROM Extension\": \"./simulation_data/ROM_Results_Extension_wMoment_1024.csv\",\n",
    "    \"ROM Lateral Bending\": \"./simulation_data/ROM_Results_LateralBending_wMoment_1024.csv\",\n",
    "    \"ROM Axial Rotation\": \"./simulation_data/ROM_Results_AxialRotation_wMoment_1024.csv\"\n",
    "}\n",
    "Y_idp_files = {\n",
    "    \"IDP Flexion\": \"./simulation_data/IDP_Results_Flexion_wMoment_1024.csv\",\n",
    "    \"IDP Extension\": \"./simulation_data/IDP_Results_Extension_wMoment_1024.csv\",\n",
    "    \"IDP Lateral Bending\": \"./simulation_data/IDP_Results_LateralBending_wMoment_1024.csv\",\n",
    "    \"IDP Axial Rotation\": \"./simulation_data/IDP_Results_AxialRotation_wMoment_1024.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model-based SA methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def load_last_column(file_path):\n",
    "    \"\"\"Loads the last column of a CSV file as a Series.\"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data.iloc[:, -1]\n",
    "\n",
    "def scale_data(df):\n",
    "    \"\"\"Applies min-max scaling to a DataFrame.\"\"\"\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "def perform_regression(X, y):\n",
    "    \"\"\"Performs linear regression and returns model, coefficients, intercept, and R² score.\"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    return model, model.coef_, model.intercept_, r2\n",
    "\n",
    "def calculate_r2_for_parameters(X, y):\n",
    "    \"\"\"Calculates R² for each parameter individually.\"\"\"\n",
    "    r2_scores = {}\n",
    "    for param in X.columns:\n",
    "        model = LinearRegression()\n",
    "        model.fit(X[[param]], y)\n",
    "        r2_scores[param] = model.score(X[[param]], y)\n",
    "    return r2_scores\n",
    "\n",
    "def calculate_pearson_correlations(X, y):\n",
    "    \"\"\"Calculates Pearson's correlation and p-value for each parameter.\"\"\"\n",
    "    correlations, p_values = {}, {}\n",
    "    for param in X.columns:\n",
    "        correlation, p_value = pearsonr(X[param], y)\n",
    "        correlations[param] = correlation\n",
    "        p_values[param] = p_value\n",
    "    return correlations, p_values\n",
    "\n",
    "def load_car_scores(filename):\n",
    "    \"\"\"Loads CAR scores from a CSV file.\"\"\"\n",
    "    return pd.read_csv(filename).iloc[:, 0].values\n",
    "\n",
    "def save_dict_to_csv(data, filename, index_name=\"Parameter\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.index.name = index_name\n",
    "    df.to_csv(filename)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed. Results are stored in the 'results' dictionary.\n"
     ]
    }
   ],
   "source": [
    "# Scale Data\n",
    "X_scaled = scale_data(X)\n",
    "Y_rom = {name: scale_data(load_last_column(path)) for name, path in Y_rom_files.items()}\n",
    "Y_idp = {name: scale_data(load_last_column(path)) for name, path in Y_idp_files.items()}\n",
    "\n",
    "# Linear Regression Analysis\n",
    "results = {}\n",
    "for name, y in {**Y_rom, **Y_idp}.items():\n",
    "    model, coefs, intercept, r2 = perform_regression(X_scaled, y)\n",
    "    r2_params = calculate_r2_for_parameters(X_scaled, y)\n",
    "    correlation, _ = calculate_pearson_correlations(X_scaled, y)\n",
    "    results[name] = {\n",
    "        \"model\": model,\n",
    "        \"coefficients\": coefs,\n",
    "        \"intercept\": intercept,\n",
    "        \"r2_score\": r2,\n",
    "        \"r2_params\": r2_params,\n",
    "        \"pearson_correlation\": correlation\n",
    "    }\n",
    "\n",
    "# Load CAR Scores for Comparison\n",
    "car_score_files = {\n",
    "    \"ROM Flexion\": \"./results_data/CARScores_ROM_flexion.csv\",\n",
    "    \"ROM Extension\": \"./results_data/CARScores_ROM_extension.csv\",\n",
    "    \"ROM Lateral Bending\": \"./results_data/CARScores_ROM_lateral_bending.csv\",\n",
    "    \"ROM Axial Rotation\": \"./results_data/CARScores_ROM_axial_rotation.csv\",\n",
    "    \"IDP Flexion\": \"./results_data/CARScores_IDP_flexion.csv\",\n",
    "    \"IDP Extension\": \"./results_data/CARScores_IDP_extension.csv\",\n",
    "    \"IDP Lateral Bending\": \"./results_data/CARScores_IDP_lateral_bending.csv\",\n",
    "    \"IDP Axial Rotation\": \"./results_data/CARScores_IDP_axial_rotation.csv\"\n",
    "}\n",
    "car_scores = {name: load_car_scores(path) for name, path in car_score_files.items()}\n",
    "\n",
    "# Summary Ratios\n",
    "for name in results:\n",
    "    r2_total = results[name][\"r2_score\"]\n",
    "    car_scores_squared = car_scores[name]**2\n",
    "    results[name][\"r2_ratios\"] = {param: r2 / r2_total for param, r2 in results[name][\"r2_params\"].items()}\n",
    "    results[name][\"car_ratios\"] = car_scores_squared / r2_total\n",
    "\n",
    "# Results contain all necessary information for further analysis or visualization\n",
    "print(\"Analysis completed. Results are stored in the 'results' dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved successfully to './results_data/' directory.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the results directory exists\n",
    "output_dir = \"./results_data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Extract and store CAR²-ratios, R² ratios, and Pearson's correlation coefficients\n",
    "car_ratios_data = {}\n",
    "r2_ratios_data = {}\n",
    "pearson_correlations_data = {}\n",
    "\n",
    "# Populate dictionaries with extracted results for each response variable\n",
    "for name, result in results.items():\n",
    "    car_ratios_data[name] = result[\"car_ratios\"]\n",
    "    r2_ratios_data[name] = result[\"r2_ratios\"]\n",
    "    pearson_correlations_data[name] = result[\"pearson_correlation\"]\n",
    "\n",
    "# Convert dictionaries to DataFrames and save as CSV files\n",
    "save_dict_to_csv(car_ratios_data, os.path.join(output_dir, \"CAR_squared_ratios.csv\"))\n",
    "save_dict_to_csv(r2_ratios_data, os.path.join(output_dir, \"COD_ratios.csv\"))\n",
    "save_dict_to_csv(pearson_correlations_data, os.path.join(output_dir, \"Pearson_correlation_coefficients.csv\"))\n",
    "\n",
    "print(\"Files saved successfully to './results_data/' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance-based SA methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup for neural network operations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters for the neural network\n",
    "hparams = {\n",
    "    'input_dim': 15,\n",
    "    'output_dim': 1,\n",
    "    'num_units': 256,\n",
    "    'num_layers': 5,\n",
    "    'dropout_p': 0\n",
    "}\n",
    "\n",
    "# Neural Network Model Definition\n",
    "class RegressionNet(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(RegressionNet, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(hparams['num_layers'] - 1):\n",
    "            i_dim = hparams['input_dim'] if i == 0 else hparams['num_units'] // (2 ** (i - 1))\n",
    "            o_dim = hparams['num_units'] // (2 ** i)\n",
    "            layers.append(nn.Linear(i_dim, o_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=hparams['dropout_p']))\n",
    "        layers.append(nn.Linear(o_dim, hparams['output_dim']))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Load Pretrained Neural Network Models\n",
    "def load_nn_model(file_path, hparams):\n",
    "    model = RegressionNet(hparams)\n",
    "    model.load_state_dict(torch.load(file_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Paths to neural network models\n",
    "nn_model_rom = load_nn_model('./models/nn_model_ROM.pth', hparams)\n",
    "nn_model_idp = load_nn_model('./models/nn_model_IDP.pth', hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobol analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sobol Analysis Setup\n",
    "problem = {\n",
    "    'num_vars': X.shape[1],  # Number of input variables,\n",
    "    'names': X.columns.tolist(),  # Input variable names\n",
    "    'bounds': [[0, 1]] * X.shape[1]  # Normalized bounds\n",
    "}\n",
    "\n",
    "num_samples = 2**13  # To reach convergence\n",
    "param_values = saltelli.sample(problem, num_samples, calc_second_order=True)\n",
    "\n",
    "# Generate Load Cases for Sobol Analysis\n",
    "def generate_load_cases(base_params):\n",
    "    load_cases = {\n",
    "        \"flexion\": np.concatenate([np.zeros((base_params.shape[0], 1)), np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"extension\": np.concatenate([np.ones((base_params.shape[0], 1)) * 1/3, np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"lateral_bending\": np.concatenate([np.ones((base_params.shape[0], 1)) * 2/3, np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"axial_rotation\": np.concatenate([np.ones((base_params.shape[0], 1)), np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1)\n",
    "    }\n",
    "    return {case: torch.tensor(params, dtype=torch.float32, device=device) for case, params in load_cases.items()}\n",
    "\n",
    "param_values_torch = generate_load_cases(param_values)\n",
    "\n",
    "# Predictions Using Neural Network Models\n",
    "def predict_with_model(model, param_values):\n",
    "    predictions = {}\n",
    "    with torch.no_grad():\n",
    "        for case, params in param_values.items():\n",
    "            predictions[case] = model(params).cpu().numpy().squeeze()\n",
    "    return predictions\n",
    "\n",
    "# Perform Sobol Analysis\n",
    "def perform_sobol_analysis(problem, predictions):\n",
    "    sobol_results = {}\n",
    "    for case, preds in predictions.items():\n",
    "        Si = sobol.analyze(problem, preds, num_resamples=1000, print_to_console=False)\n",
    "        sobol_results[case] = Si\n",
    "    return sobol_results\n",
    "\n",
    "# Store Sobol Results\n",
    "def sobol_results_to_csv(sobol_results, metric):\n",
    "    output_dir = './results_data/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for case, Si in sobol_results.items():\n",
    "        # Single-order and total-order indices\n",
    "        df_1d = pd.DataFrame({\n",
    "            'S1': Si['S1'],\n",
    "            'S1_conf': Si['S1_conf'],\n",
    "            'ST': Si['ST'],\n",
    "            'ST_conf': Si['ST_conf'],\n",
    "        }, index=problem['names'])\n",
    "        \n",
    "        # Second-order indices\n",
    "        df_s2 = pd.DataFrame(Si['S2'], index=problem['names'], columns=problem['names'])\n",
    "        df_s2_conf = pd.DataFrame(Si['S2_conf'], index=problem['names'], columns=problem['names'])\n",
    "        \n",
    "        # Paths for saving the results with \"sobol\" in the filename\n",
    "        path_s1st = f\"{output_dir}SobolS1ST_{metric}_{case}.csv\"\n",
    "        path_s2 = f\"{output_dir}SobolS2_{metric}_{case}.csv\"\n",
    "        path_s2_conf = f\"{output_dir}SobolS2conf_{metric}_{case}.csv\"\n",
    "        \n",
    "        # Save dataframes to CSV\n",
    "        df_1d.to_csv(path_s1st)\n",
    "        df_s2.to_csv(path_s2)\n",
    "        df_s2_conf.to_csv(path_s2_conf)\n",
    "\n",
    "# Execute Sobol Analysis for ROM and IDP Metrics\n",
    "rom_predictions = predict_with_model(nn_model_rom, param_values_torch)\n",
    "idp_predictions = predict_with_model(nn_model_idp, param_values_torch)\n",
    "\n",
    "rom_sobol_results = perform_sobol_analysis(problem, rom_predictions)\n",
    "idp_sobol_results = perform_sobol_analysis(problem, idp_predictions)\n",
    "\n",
    "# Save results to CSV files\n",
    "sobol_results_to_csv(rom_sobol_results, \"ROM\")\n",
    "sobol_results_to_csv(idp_sobol_results, \"IDP\")\n",
    "\n",
    "print(\"Sobol analysis completed. Results saved to './results_data/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eFAST analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eFAST analysis completed. Results saved to './results_data/'.\n"
     ]
    }
   ],
   "source": [
    "# Define eFAST Analysis Problem Setup\n",
    "problem = {\n",
    "    'num_vars': X.shape[1],  # Number of input variables,\n",
    "    'names': X.columns.tolist(),  # Input variable names\n",
    "    'bounds': [[0, 1]] * X.shape[1]  # Normalized bounds\n",
    "}\n",
    "\n",
    "# Number of samples for eFAST\n",
    "M_fast = 4\n",
    "num_samples = 2**13  # Ensures sufficient convergence\n",
    "\n",
    "# Generate eFAST Samples\n",
    "param_values = fast_sampler.sample(problem, num_samples, M=M_fast, seed=4)\n",
    "\n",
    "# Generate Load Cases for eFAST Analysis\n",
    "def generate_load_cases(base_params):\n",
    "    \"\"\"Generates parameters for each load case.\"\"\"\n",
    "    load_cases = {\n",
    "        \"flexion\": np.concatenate([np.zeros((base_params.shape[0], 1)), np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"extension\": np.concatenate([np.ones((base_params.shape[0], 1)) * 1/3, np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"lateral_bending\": np.concatenate([np.ones((base_params.shape[0], 1)) * 2/3, np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1),\n",
    "        \"axial_rotation\": np.concatenate([np.ones((base_params.shape[0], 1)), np.ones((base_params.shape[0], 1)) * 5, base_params], axis=1)\n",
    "    }\n",
    "    return {case: torch.tensor(params, dtype=torch.float32, device=device) for case, params in load_cases.items()}\n",
    "\n",
    "# Device setup for neural network operations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load cases for eFAST\n",
    "param_values_torch = generate_load_cases(param_values)\n",
    "\n",
    "# Predictions Using Neural Network Models\n",
    "def predict_with_model(model, param_values):\n",
    "    \"\"\"Generates predictions for each load case using the neural network model.\"\"\"\n",
    "    predictions = {}\n",
    "    with torch.no_grad():\n",
    "        for case, params in param_values.items():\n",
    "            predictions[case] = model(params).cpu().numpy().squeeze()\n",
    "    return predictions\n",
    "\n",
    "# Perform eFAST Analysis\n",
    "def perform_fast_analysis(problem, predictions):\n",
    "    \"\"\"Performs eFAST analysis and stores results for each load case.\"\"\"\n",
    "    fast_results = {}\n",
    "    for case, preds in predictions.items():\n",
    "        Si = fast.analyze(problem, preds, M=M_fast, num_resamples=1000, conf_level=0.95, print_to_console=False, seed=4)\n",
    "        fast_results[case] = Si\n",
    "    return fast_results\n",
    "\n",
    "# Store eFAST Results\n",
    "def fast_results_to_csv(fast_results, metric):\n",
    "    \"\"\"Converts and saves eFAST analysis results to CSV files.\"\"\"\n",
    "    output_dir = './results_data/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for case, Si in fast_results.items():\n",
    "        # First-order and total-order indices\n",
    "        df_1d = pd.DataFrame({\n",
    "            'S1': Si['S1'],\n",
    "            'S1_conf': Si['S1_conf'],\n",
    "            'ST': Si['ST'],\n",
    "            'ST_conf': Si['ST_conf'],\n",
    "        }, index=problem['names'])\n",
    "        \n",
    "        # Path for saving the results\n",
    "        path_s1st = f\"{output_dir}EFAST_{metric}_{case}.csv\"\n",
    "        \n",
    "        # Save dataframe to CSV\n",
    "        df_1d.to_csv(path_s1st)\n",
    "\n",
    "# Execute eFAST Analysis for ROM and IDP Metrics\n",
    "rom_predictions = predict_with_model(nn_model_rom, param_values_torch)\n",
    "idp_predictions = predict_with_model(nn_model_idp, param_values_torch)\n",
    "\n",
    "rom_fast_results = perform_fast_analysis(problem, rom_predictions)\n",
    "idp_fast_results = perform_fast_analysis(problem, idp_predictions)\n",
    "\n",
    "# Save results to CSV files\n",
    "fast_results_to_csv(rom_fast_results, \"ROM\")\n",
    "fast_results_to_csv(idp_fast_results, \"IDP\")\n",
    "\n",
    "print(\"eFAST analysis completed. Results saved to './results_data/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBD-FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBD-FAST analysis completed. Results saved in './results_data/'.\n"
     ]
    }
   ],
   "source": [
    "# Define the RBD-FAST Analysis Problem Setup\n",
    "problem = {\n",
    "    'num_vars': X.shape[1],  # Number of input variables,\n",
    "    'names': X.columns.tolist(),  # Input variable names\n",
    "    'bounds': [[0, 1]] * X.shape[1]  # Normalized bounds\n",
    "}\n",
    "\n",
    "# Load response variables for ROM and IDP\n",
    "Y_rom = {case: pd.read_csv(path).iloc[:, -1] for case, path in Y_rom_files.items()}\n",
    "Y_idp = {case: pd.read_csv(path).iloc[:, -1] for case, path in Y_idp_files.items()}\n",
    "\n",
    "# Perform RBD-FAST Analysis\n",
    "def perform_rbd_fast_analysis(problem, X, y_dict):\n",
    "    \"\"\"Performs RBD-FAST analysis for each response variable in different load cases.\"\"\"\n",
    "    rbd_fast_results = {}\n",
    "    for case, y in y_dict.items():\n",
    "        Si = rbd_fast.analyze(problem, X=X.values, Y=y.values, print_to_console=False)\n",
    "        rbd_fast_results[case] = Si\n",
    "    return rbd_fast_results\n",
    "\n",
    "# Store RBD-FAST Results\n",
    "def rbd_fast_results_to_csv(rbd_fast_results, metric):\n",
    "    \"\"\"Saves RBD-FAST results to CSV files for each load case.\"\"\"\n",
    "    output_dir = './results_data/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for case, Si in rbd_fast_results.items():\n",
    "        # Extract load case by removing the metric prefix (\"ROM\" or \"IDP\") and joining the rest\n",
    "        load_case = \"_\".join(case.split()[1:]).lower().replace(\" \", \"\")  # Preserve full load case\n",
    "        \n",
    "        # Create the filename with the desired format\n",
    "        filename = f\"RBDFAST_{metric}_{load_case}.csv\"\n",
    "        \n",
    "        df_results = pd.DataFrame({\n",
    "            'S1': Si['S1'],\n",
    "            'S1_conf': Si['S1_conf']\n",
    "        }, index=problem['names'])\n",
    "\n",
    "        # Save results to the specified path\n",
    "        df_results.to_csv(os.path.join(output_dir, filename))\n",
    "\n",
    "# Execute RBD-FAST Analysis for ROM and IDP Metrics\n",
    "rom_rbd_fast_results = perform_rbd_fast_analysis(problem, X, Y_rom)\n",
    "idp_rbd_fast_results = perform_rbd_fast_analysis(problem, X, Y_idp)\n",
    "\n",
    "# Save results to CSV files\n",
    "rbd_fast_results_to_csv(rom_rbd_fast_results, \"ROM\")\n",
    "rbd_fast_results_to_csv(idp_rbd_fast_results, \"IDP\")\n",
    "\n",
    "print(\"RBD-FAST analysis completed. Results saved in './results_data/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-based IG method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG analysis completed. Results saved in './results_data/'.\n"
     ]
    }
   ],
   "source": [
    "# Define input feature names for the model\n",
    "FEATURES = ['LoadCase', 'Moment', 'C10Nucleus', 'C01Nucleus', 'C10Annulus', 'K1Annulus', 'K2Annulus', 'Kappa', \n",
    "                    'K1Circ', 'K2Circ', 'K1Rad', 'K2Rad', 'FiberAngle', 'FiberAngleCirc', 'FiberAngleRad']\n",
    "\n",
    "# Define load cases and their corresponding normalized values\n",
    "load_cases = ['Flexion', 'Extension', 'Lateral_Bending', 'Axial_Rotation']\n",
    "load_cases_assigned_values = [0, 2/3, 1, 1/3]\n",
    "\n",
    "# Load normalized data for the IG method\n",
    "df_trainval = pd.read_csv(\"./simulation_data/trainval_df_large_rom_norm.csv\")\n",
    "df_test = pd.read_csv(\"./simulation_data/test_df_large_rom_norm.csv\")\n",
    "\n",
    "def integrated_gradients(model, input_sample, baseline, steps=50):\n",
    "    \"\"\"Computes Integrated Gradients (IG) for a given input sample.\"\"\"\n",
    "    # Generate scaled inputs\n",
    "    scaled_inputs = [(baseline + float(i) / steps * (input_sample - baseline)).unsqueeze(0)\n",
    "                     for i in range(0, steps + 1)]\n",
    "\n",
    "    # Compute gradients for each scaled input\n",
    "    grads = []\n",
    "    for scaled_input in scaled_inputs:\n",
    "        scaled_input = scaled_input.clone().detach().requires_grad_(True)  \n",
    "        output = model(scaled_input)\n",
    "        grad = torch.autograd.grad(outputs=output, inputs=scaled_input, create_graph=True)[0]\n",
    "        grads.append(grad)\n",
    "\n",
    "    # Average the gradients\n",
    "    avg_grads = torch.mean(torch.stack(grads), dim=0)\n",
    "\n",
    "    # Integrated gradients approximation\n",
    "    integrated_grads = (input_sample - baseline) * avg_grads\n",
    "\n",
    "    return integrated_grads\n",
    "\n",
    "# Function to calculate and save IG for a specific model and metric\n",
    "def calculate_ig_for_metric(model, df_trainval, df_test, metric_name):\n",
    "    \"\"\"Calculates and stores Integrated Gradients (IG) sensitivity values for a specific metric (ROM/IDP).\"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    \n",
    "    results = {}\n",
    "    for load_case, value in zip(load_cases, load_cases_assigned_values):\n",
    "        # Filter test samples for the current load case and moment value\n",
    "        samples = df_test[np.isclose(df_test.LoadCase, value) & np.isclose(df_test.Moment, 1)]\n",
    "        samples = samples.drop(['config_id', 'y_ROM', 'y_IDP'], axis=1)\n",
    "        \n",
    "        # Convert test samples to a tensor\n",
    "        inputs = torch.tensor(samples.values, dtype=torch.float32, requires_grad=True)\n",
    "        num_samples = len(inputs)\n",
    "        \n",
    "        # Compute the baseline as the mean of training/validation data for the current load case\n",
    "        baseline = df_trainval[np.isclose(df_trainval.LoadCase, value) & np.isclose(df_trainval.Moment, 1)]\n",
    "        baseline = baseline.drop(['config_id', 'y_ROM', 'y_IDP'], axis=1).mean().values\n",
    "        baseline = torch.tensor(baseline, dtype=torch.float32)\n",
    "        baseline[0] = value # Set baseline LoadCase value\n",
    "        baseline[1] = 1 # Set baseline Moment value\n",
    "        \n",
    "        # Initialize tensor to store IG values for each sample\n",
    "        integrated_grads = torch.zeros(num_samples, len(FEATURES))\n",
    "        for i in range(num_samples):\n",
    "            input_sample = inputs[i]\n",
    "            ig = integrated_gradients(model, input_sample, baseline) # Compute IG\n",
    "            integrated_grads[i] = ig\n",
    "        \n",
    "        # Compute the average absolute IG values across all test samples\n",
    "        avg_integrated_sensitivity = torch.mean(integrated_grads.abs(), dim=0)[2:]  # Skip LoadCase and Moment\n",
    "        normalized_sensitivity = avg_integrated_sensitivity / torch.sum(avg_integrated_sensitivity)\n",
    "        \n",
    "        # Store normalized sensitivities in the results dictionary\n",
    "        results[load_case] = {FEATURES[i + 2]: normalized_sensitivity[i].item() for i in range(len(normalized_sensitivity))}\n",
    "    \n",
    "    # Save results to CSV\n",
    "    output_dir = './results_data/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for load_case in results:\n",
    "        pd.DataFrame.from_dict(results[load_case], orient='index', columns=[f'{metric_name} Sensitivity']).to_csv(\n",
    "            os.path.join(output_dir, f'IG_{metric_name}_{load_case.lower()}.csv'))\n",
    "\n",
    "# Calculate IG for ROM and IDP\n",
    "calculate_ig_for_metric(nn_model_rom, df_trainval, df_test, \"ROM\")\n",
    "calculate_ig_for_metric(nn_model_idp, df_trainval, df_test, \"IDP\")\n",
    "\n",
    "print(\"IG analysis completed. Results saved in './results_data/'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensitivity_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
